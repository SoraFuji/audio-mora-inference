# audio-mora-inference
概要

このリポジトリは、環境音WAVを一定時間ごとに分割し、各区間について音響埋め込み（YAMNet）を用いてファインチューニング済み分類モデルで推論し、Top-5のクラス候補を表示するデモです。
※本デモは 推論専用です。学習データは含まれておらず、推論の実行にも不要です。

使い方（コードを書かない手順）

GitHub上で notebook/inference_pipeline.ipynb を開きます。

ノートブック上部に表示される 「Open in Colab」 をクリックします。

Google Colab がブラウザ上で起動するので、ノートブックを 上から順に実行します。

実行中に表示される指示に従って、推論したい WAVファイルをアップロードします。

音声は自動的に一定時間（例：5秒）ごとに分割され、各区間について推論結果（Top-5 クラス）が表示されます。各区間の音声再生と、Top-5の棒グラフも表示されます。

仕様

入力：WAVファイル（推奨：モノラル、サンプリングレート16kHz相当。異なる場合もColab内で読み込み時に変換されます）

分割：一定時間ごと（例：5秒）に分割して区間ごとに推論

Speech除外：YAMNetでSpeech確率を推定し、閾値を超える区間は推論をスキップ

特徴量：YAMNet埋め込み（1024次元）の mean / std / max を結合した 3072次元ベクトル

出力：非Speech区間についてTop-5クラス（ラベル名と確率）

注意点（重要）

入力はWAVファイルのみ対応です（mp3等は対象外。必要ならWAVに変換してください）。

推論結果のラベル表示は labels.txt を参照します。labels.txt の行数と順序は、モデルの出力順と一致している必要があります。 一致しない場合、推論は動作してもラベル名がずれて表示されます。

人の声（Speech）と判定された区間は、設定した閾値により 推論がスキップされます（Top-5が空になります）。

初回実行時は、Colab上でライブラリやYAMNetの読み込みが行われるため、表示まで少し時間がかかることがあります。

ファイル名に特殊文字が含まれる場合、環境によっては扱いにくいことがあります。困った場合は英数字のファイル名に変更してください。
